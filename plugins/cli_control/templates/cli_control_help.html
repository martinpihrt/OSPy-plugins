$def with()

$var title: $_('CLI Control settings - help')
$var page: plugins

<div id="plugin">
    <div class="title">$_('CLI Control settings - help')</div>
    <p>$_('This plugin can be used to control devices such as remote RF units via Linux commands.')</p>
    <p>$_('Enter command line commands to be issued when a station is to be started or stopped.')</p>
    <p>$_('For example a command might look like: wget http://xxx.xxx.xxx.xxx/relay1on')</p> 
    <p>$_('Leave fields blank for any stations not to be controlled from the command line.')</p></br>
    <p><b>$_('Sensors')</b></p>
    <p>$_('Example of a command to control a relay on the sensor board.')</p>
    <p>$_('Switching ON the station without time limitation (the sensor relay remains closed until a switch-off command arrives).')</p>
    <ul><li><p>$_('wget --post-data "re=1" http://ip:port/sensorpassword')</p></li></ul>
    <p>$_('Switching ON the station with time limitation (the sensor relay switches off after this time regardless of whether a switch-off command arrives).')</p>
    <ul><li><p>$_('wget --post-data "re=1&run=480" http://10.10.10.34:80/0123456789abcdef')</p></li></ul>
    <p>$_('Switching OFF the station.')</p>
    <ul><li><p>$_('wget --post-data "re=0" http://ip:port/sensorpassword')</p></li></ul>
    <p>$_('In the case of a command written in this way, after executing this command, the output will be saved to a file in the root of the OSPy folder (example of the name of the created file: 0123456789abcdef?re=1). If we do not need to save the output, we will use another parameter in wget command.')</p></br>
    <p><b>$_('Description WGET')</b></p>
    <p>$_('WGET is a free utility for non-interactive download of files from the web. It supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies. wget is non-interactive, meaning that it can work in the background, while the user is not logged on, which allows you to start a retrieval and disconnect from the system, letting wget finish the work. By contrast, most web browsers require constant user interaction, which make transferring a lot of data difficult. wget can follow links in HTML and XHTML pages and create local versions of remote websites, fully recreating the directory structure of the original site, which is sometimes called "recursive downloading." While doing that, wget respects the Robot Exclusion Standard (robots.txt). wget can be instructed to convert the links in downloaded HTML files to the local files for offline viewing. wget has been designed for robustness over slow or unstable network connections; if a download fails due to a network problem, it will keep retrying until the whole file has been retrieved. If the server supports regetting, it will instruct the server to continue the download from where it left off.')</p></br>
    <p><b>$_('Syntax WGET')</b></p>
    <p>$_('wget [option]... [URL]...')</p>
    <p>$_('Basic startup options')</p>
    <p>$_('-V, --version Display the version of wget, and exit.')</p>
    <p>$_('-h, --help Print a help message describing all of wgets command-line options, and exit.')</p>
    <p>$_('-b, --background Go to background immediately after startup. If no output file is specified via the -o, output is redirected to wget-log.')</p>
    <p>$_('-e command, --execute command Execute command as if it were a part of the file .wgetrc. A command thus invoked will be executed after the commands in .wgetrc, thus taking precedence over them.')</p></br>
    <p>$_('Logging and input file options')</p>
    <p>$_('-o logfile, --output-file=logfile Log all messages to logfile. The messages are normally reported to standard error.')</p>
    <p>$_('-a logfile, --append-output=logfile Append to logfile. This option is the same as -o, only it appends to logfile instead of overwriting the old log file. If logfile does not exist, a new file is created.')</p>
    <p>$_('-d, --debug Turn on debug output, meaning various information important to the developers of wget if it does not work properly. Your system administrator may have chosen to compile wget without debug support, in which case -d will not work. Note that compiling with debug support is always safe; wget compiled with the debug support will not print any debug info unless requested with -d.')</p>
    <p>$_('-q, --quiet Turn off wgets output.')</p>
    <p>$_('-v, --verbose Turn on verbose output, with all the available data. The default output is verbose.')</p>
    <p>$_('-nv, --non-verbose  Non-verbose output. Turn off verbose without being completely quiet (use -q for that), which means that error messages and basic information still get printed.')</p>
    <p>$_('-i file, --input-file=file  Read URLs from a local or external file. If "-" is specified as file, URLs are read from the standard input. (Use "./-" to read from a file literally named "-".')</p>
    <p>$_('If this function is used, no URLs need be present on the command line. If there are URLs both on the command line and input file, those on the command lines will be the first ones to be retrieved. If --force-html is not specified, then file should consist of a series of URLs, one per line.')</p>
    <p>$_('However, if you specify --force-html, the document will be regarded as HTML. In that case you may have problems with relative links, which you can solve either by adding <base href="url"> to the documents or by specifying --base=url on the command line.')</p>
    <p>$_('If the file is an external one, the document will be automatically treated as HTML if the Content-Type is "text/html". Furthermore, the files location will be implicitly used as base href if none was specified.')</p>
    <p>$_('-F, --force-html When input is read from a file, force it to be treated as an HTML file. This enables you to retrieve relative links from existing HTML files on your local disk, by adding base href="url" to HTML, or using the --base command-line option.')</p>
    <p>$_('-B URL --base=URL Resolves relative links using URL as the point of reference, when reading links from an HTML file specified via the -i/--input-file option (together with --force-html, or when the input file was fetched remotely from a server describing it as HTML). This option is equivalent to the presence of a "BASE" tag in the HTML input file, with URL as the value for the "href" attribute.')</p>
    <p>$_('For instance, if you specify http://foo/bar/a.html for URL, and wget reads ../baz/b.html from the input file, it would be resolved to http://foo/baz/b.html.')</p>
    <p>$_('--config=FILE Specify the location of a startup file you want to use.')</p></br>
    <p>$_('Directory options')</p>
    <p>$_('-nd, --no-directories Do not create a hierarchy of directories when retrieving recursively. With this option turned on, all files will get saved to the current directory, without clobbering (if a name shows up more than once, the file names will get extensions .n).')</p>
    <p>$_('-x, --force-directories The opposite of -nd; create a hierarchy of directories, even if one would not have been created otherwise. For example, wget -x http://fly.srk.fer.hr/robots.txt will save the downloaded file to fly.srk.fer.hr/robots.txt.')</p>
    <p>$_('-nH, --no-host-directories   Disable generation of host-prefixed directories. By default, invoking wget with -r http://fly.srk.fer.hr/ will create a structure of directories beginning with fly.srk.fer.hr/. This option disables such behavior.')</p>
    <p>$_('--protocol-directories  Use the protocol name as a directory component of local file names. For example, with this option, wget -r http://host will save to http/host/... rather than just to host/....')</p>
    <p>$_('--cut-dirs=number   Ignore number directory components. This option is useful for getting a fine-grained control over the directory where recursive retrieval will be saved.')</p>
    <p>$_('Take, for example, the directory at ftp://ftp.xemacs.org/pub/xemacs/. If you retrieve it with -r, it will be saved locally under ftp.xemacs.org/pub/xemacs/. While the -nH option can remove the ftp.xemacs.org/ part, you are still stuck with pub/xemacs, which is where --cut-dirs comes in handy; it makes wget not "see" number remote directory components. Here are several examples of how --cut-dirs option works:')</p>
    <p>$_('(no options) ftp.xemacs.org/pub/xemacs/')</p>
    <p>$_('-nH pub/xemacs/')</p>
    <p>$_('-nH --cut-dirs=1 xemacs/')</p>
    <p>$_('-nH --cut-dirs=2 .')</p>
    <p>$_('--cut-dirs=1 ftp.xemacs.org/xemacs/')</p>
    <p>$_('If you just want to get rid of the directory structure, this option is similar to a combination of -nd and -P. However, unlike -nd, --cut-dirs does not lose with subdirectories; for instance, with -nH --cut-dirs=1, a beta/ subdirectory will be placed to xemacs/beta, as one would expect.')</p>
    <p>$_('-P prefix, --directory-prefix=prefix Set directory prefix to prefix. The directory prefix is the directory where all other files and subdirectories will be saved to, i.e.the top of the retrieval tree. The default is "." (the current directory).')</p></br>
    <p>$_('HTTP options')</p>
    <p>$_('--http-user=user, --http-passwd=password  Specify the username user and password on an HTTP server. According to the challenge, wget will encode them using either the "basic" (insecure) or the "digest" authentication scheme.')</p>
    <p>$_('Another way to specify username and password is in the URL itself. Either method reveals your password to anyone who bothers to run ps. To prevent the passwords from being seen, store them in .wgetrc or .netrc, and make sure to protect those files from other users with chmod. If the passwords are important, do not leave them lying in those files either edit the files and delete them after wget has started the download.')</p>
    <p>$_('--no-cache  Disable server-side cache. In this case, wget will send the remote server an appropriate directive (Pragma: no-cache) to get the file from the remote service, rather than returning the cached version. This option is especially useful for retrieving and flushing out-of-date documents on proxy servers.')</p>
    <p>$_('Caching is allowed by default.')</p>
    <p>$_('--no-cookies Disable the use of cookies. Cookies are a mechanism for maintaining server-side state. The server sends the client a cookie using the "Set-Cookie" header, and the client responds with the same cookie upon further requests. Since cookies allow the server owners to keep track of visitors and for sites to exchange this information, some consider them a breach of privacy. The default is to use cookies however, storing cookies is not on by default.')</p>
    <p>$_('--load-cookies file Load cookies from file before the first HTTP retrieval. file is a text file in the format originally used by Netscapes cookies.txt file.')</p>
    <p>$_('You will typically use this option when mirroring sites that require that you be logged in to access some or all of their content. The login process typically works by the web server issuing an HTTP cookie upon receiving and verifying your credentials. The cookie is then resent by the browser when accessing that part of the site, and so proves your identity.')</p>
    <p>$_('Mirroring such a site requires wget to send the same cookies your browser sends when communicating with the site. To do this use --load-cookies; point wget to the location of the cookies.txt file, and it will send the same cookies your browser would send in the same situation. Different browsers keep text cookie files in different locations:')</p>
    <p>$_('Netscape 4.x The cookies are in ~/.netscape/cookies.txt.')</p>
    <p>$_('Mozilla and Netscape 6.x Mozillas cookie file is also named cookies.txt, located somewhere under ~/.mozilla, in the directory of your profile. The full path usually ends up looking somewhat like ~/.mozilla/default/some-weird-string/cookies.txt.')</p>
    <p>$_('Internet Explorer   You can produce a cookie file that wget can utilize using the File menu, Import and Export, Export Cookies. Tested with Internet Explorer 5 (wow. thats old), but it is not guaranteed to work with earlier versions. Other browsers  If you are using a different browser to create your cookies, --load-cookies only works if you can locate or produce a cookie file in the Netscape format that wget expects.')</p>
    <p>$_('If you cannot use --load-cookies, there might still be an alternative. If your browser supports a "cookie manager", you can use it to view the cookies used when accessing the site youre mirroring. Write down the name and value of the cookie, and manually instruct wget to send those cookies, bypassing the "official" cookie support:')</p>
    <p>$_('wget --no-cookies --header "Cookie: <name>=<value>"')</p>
    <p>$_('--save-cookies file Save cookies to file before exiting. This will not save cookies that have expired or that have no expiry time (so-called "session cookies"), but also see --keep-session-cookies.')</p>
    <p>$_('--keep-session-cookies  When specified, causes --save-cookies to also save session cookies. Session cookies are normally not saved because they are meant to be kept in memory and forgotten when you exit the browser. Saving them is useful on sites that require you to log in or to visit the homepage before you can access some pages. With this option, multiple wget runs are considered a single browser session as far as the site is concerned.')</p>

    <p>$_('Since the cookie file format does not normally carry session cookies, wget marks them with an expiry timestamp of 0. wgets --load-cookies recognizes those as session cookies, but it might confuse other browsers. Also, note that cookies so loaded will be treated as other session cookies, which means that if you want --save-cookies to preserve them again, you must use --keep-session-cookies again.')</p>
    <p>$_('--ignore-length Unfortunately, some HTTP servers (CGI programs, to be more precise) send out bogus "Content-Length" headers, which makes wget start to bray like a stuck pig, as it thinks not all the document was retrieved. You can spot this syndrome if wget retries getting the same document again and again, each time claiming that the (otherwise normal) connection has closed on the very same byte.')</p>
    <p>$_('With this option, wget ignores the "Content-Length" header, as if it never existed.')</p>
    <p>$_('--header=header-line Send header-line along with the rest of the headers in each HTTP request. The supplied header is sent as-is, which means it must contain name and value separated by colon, and must not contain newlines.')</p>
    <p>$_('You may define more than one additional header by specifying --header more than once.')</p>
    <p>$_('wget --header="Accept-Charset: iso-8859-2"  --header="Accept-Language: hr"  http://fly.srk.fer.hr/')</p>
    <p>$_('Specification of an empty string as the header value will clear all previous user-defined headers.')</p>
    <p>$_('As of wget 1.10, this option can be used to override headers otherwise generated automatically. This example instructs wget to connect to localhost, but to specify foo.bar in the "Host" header:')</p>
    <p>$_('wget --header="Host: foo.bar" http://localhost/')</p>
    <p>$_('In versions of wget prior to 1.10 such use of --header caused sending of duplicate headers.')</p>
    <p>$_('--max-redirect=number   Specifies the maximum number of redirections to follow for a resource. The default is 20, which is usually far more than necessary. However, on those occasions where you want to allow more (or fewer), this is the option to use.')</p>
    <p>$_('--proxy-user=user, --proxy-password=password   Specify the username user and password for authentication on a proxy server. wget will encode them using the "basic" authentication scheme.')</p>
    <p>$_('Security considerations similar to those with --http-password pertain here as well.')</p>
    <p>$_('--referer=url   Include "Referer: url" header in HTTP request. Useful for retrieving documents with server-side processing that assume they are always being retrieved by interactive web browsers and only come out properly when Referer is set to one of the pages that point to them.')</p>
    <p>$_('--save-headers  Save the headers sent by the HTTP server to the file, preceding the actual contents, with an empty line as the separator.')</p>
    <p>$_('-U agent-string, --user-agent=agent-string   Identify as agent-string to the HTTP server.')</p>
    <p>$_('The HTTP protocol allows the clients to identify themselves using a "User-Agent" header field. This enables distinguishing the WWW software, usually for statistical purposes or for tracing of protocol violations. wget normally identifies as "Wget/version", version being the current version number of wget.')</p>
    <p>$_('However, some sites have been known to impose the policy of tailoring the output according to the "User-Agent"-supplied information. While this is not such a bad idea in theory, it has been abused by servers denying information to clients other than (historically) Netscape or, more frequently, Microsoft Internet Explorer. This option allows you to change the "User-Agent" line issued by wget. Use of this option is discouraged, unless you really know what you are doing.')</p>
    <p>$_('Specifying empty user agent with --user-agent="" instructs wget not to send the "User-Agent" header in HTTP requests.')</p>
    <p>$_('--post-data=string, --post-file=file    Use POST as the method for all HTTP requests and send the specified data in the request body. --post-data sends string as data, whereas --post-file sends the contents of file. Other than that, they work in exactly the same way. In particular, they both expect content of the form "key1=value1&key2=value2", with percent-encoding for special characters; the only difference is that one expects its content as a command-line parameter and the other accepts its content from a file. In particular, --post-file is not for transmitting files as form attachments: those must appear as "key=value" data (with appropriate percent-coding) just like everything else. wget does not currently support "multipart/form-data" for transmitting POST data; only "application/x-www-form-urlencoded". Only one of --post-data and --post-file should be specified.')</p>
    <p>$_('Please be aware that wget needs to know the size of the POST data in advance. Therefore the argument to "--post-file" must be a regular file; specifying a FIFO or something like /dev/stdin wont work. Its not quite clear how to work around this limitation inherent in HTTP/1.0. Although HTTP/1.1 introduces chunked transfer that doesnt require knowing the request length in advance, a client cant use chunked unless it knows its talking to an HTTP/1.1 server. And it cant know that until it receives a response, which in turn requires the request to have been completed, which is sort of a chicken-and-egg problem.')</p>
    <p>$_('Note that if wget is redirected after the POST request is completed, it will not send the POST data to the redirected URL. Because URLs that process POST often respond with a redirection to a regular page, which does not desire or accept POST. It is not completely clear that this behavior is optimal; if it doesnt work out, it might be changed in the future.')</p>
    <p>$_('This example shows how to log to a server using POST and then proceed to download the desired pages, presumably only accessible to authorized users. First, we log in to the server, which can be done only once.')</p>
    <p>$_('wget --save-cookies cookies.txt --post-data "user=foo&password=bar" http://server.com/auth.php')</p>
    <p>$_('And then we grab the page (or pages) we care about:')</p>
    <p>$_('wget --load-cookies cookies.txt  -p http://server.com/interesting/article.php')</p>
    <p>$_('If the server is using session cookies to track user authentication, the above will not work because --save-cookies will not save them (and neither will browsers) and the cookies.txt file will be empty. In that case use --keep-session-cookies along with --save-cookies to force saving of session cookies.')</p>
    <p>$_('--content-disposition   If this is set, experimental (not fully-functional) support for "Content-Disposition" headers is enabled. This option can currently result in extra round-trips to the server for a "HEAD" request, and is known to suffer from a few bugs, which is why it is not currently enabled by default.')</p>
    <p>$_('This option is useful for some file-downloading CGI programs that use "Content-Disposition" headers to describe what the name of a downloaded file should be.')</p>
    <p>$_('--trust-server-names If this is set, on a redirect the last component of the redirection URL will be used as the local file name. By default, it is used the last component in the original URL.')</p>
    <p>$_('--auth-no-challenge If this option is given, wget will send Basic HTTP authentication information (plaintext username and password) for all requests, just like wget 1.10.2 and prior did by default.')</p>
    <p>$_('Use of this option is not recommended, and is intended only to support some few obscure servers, which never send HTTP authentication challenges, but accept unsolicited auth info, say, in addition to form-based authentication.')</p>    
    <p>$_('Source taken from the web') <a href="https://www.computerhope.com/unix/wget.htm">$_('www.computerhope.com')</a><br/>
</div>

<div id="controls">
    <a href=$plugins.plugin_url('cli_control.settings_page') class="button danger">$_('Cancel')</a><br/>
</div>